---
title: 'Exercise #6'
author: "Brian Schweigler; 16-102-071"
date: "27/04/2022"
output:
  pdf_document: default
  html_document: default
subtitle: Model Evaluation with R
editor_options: 
  markdown: 
    wrap: 72
---
## Preliminaries
Load the required libraries

```{r}
library(dplyr)
library(tidyr)
library(ggplot2)
library(FNN)
```


To normalize data, we define the following function:
```{r}
normalize = function(x) {
	(x - min(x)) / (max(x) - min(x))
}
```

We define the best model as the one with the lowest MSE.
```{r}
best_k_for_knn_reg = function(train,
															train_labels,
															test,
															test_labels,
															kStart,
															kEnd) {
	best_mse = NA
	for (k in kStart:kEnd) {
		model = knn.reg(
			train = train,
			test = test,
			y = train_labels,
			k = k
		)
		
		mse = mean((model$pred - test_labels) ^ 2)
		
		if (is.na(best_mse) || mse < best_mse) {
			best_mse = mse
			best_k = k
		}
	}
	
	return(best_k)
}
```


Loading the computers dataset:

```{r}
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))

computers_df = read.csv("Computers.txt", header = TRUE, sep = "\t"  , comment.char = "#")
```

As we know from exercise 4, Computers.txt has no outliers discernible (without additional information). 

Loading the cars dataset and cleaning NAs:

```{r}
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))

cars_df = read.csv("Cars.txt", header = TRUE, sep = "\t", comment.char = "#")
```
There are 6 NAs for horsepower that need to be removed -> delete the whole corresponding rows.
These are also mentioned in Cars.pdf.
```{r}
cars_df_cleaned <- cars_df[!is.na(cars_df$horsepower),]
```
Besides the NAs found for horspower, the data seems to be good (without additional information.)


\newpage

## 1. Compare the two models (linear regression VS k-NN) for the Computers dataset (filename: Computers.txt) as it follows:

## 1a. Evaluate the quality of the fit (of the best model) between a single regression model of your choice and a multiple regression.

First exclude vendor, model and ERP, then normalize:
```{r}
computers_df = computers_df[, 3:9]
computers_df_norm = as.data.frame(lapply(computers_df, normalize))
summary(computers_df_norm)
```

Now we create the two regression models:
```{r}
lm_computers_single = lm(PRP ~ MMAX, data = computers_df_norm)
summary(lm_computers_single)
lm_computers_multiple = lm(PRP ~ MYCT + MMIN + MMAX + CACH + CGMIN + CHMAX, data = computers_df_norm)
summary(lm_computers_multiple)
```

Compare these by calculating MSE:
```{r}
computers_predict_single = cbind(computers_df_norm,
																 predict(lm_computers_single, interval = 'confidence'))
computers_predict_multiple = cbind(computers_df_norm,
																	 predict(lm_computers_multiple, interval = 'confidence'))
mse_computers_single_linear = mean((computers_predict_single$PRP - computers_predict_single$fit) ^
																	 	2)
mse_computers_multi_linear = mean((
	computers_predict_multiple$PRP - computers_predict_multiple$fit
) ^ 2)
print(sprintf("Single linear regression MSE = %f", mse_computers_single_linear))
print(sprintf(
	"Multiple linear regression MSE = %f",
	mse_computers_multi_linear
))
```

As expected, the multiple-regression model performs better (0.0027 vs 0.0050).



## 1b. Use the k-NN regression to build the second model, applying LOO or 10fold cross-validation

We will be using 10-fold cross-validation:
```{r}
x = 10
n = nrow(computers_df_norm)
chunkSize = floor(n / x)
meanMSE = 0.0
indexRange = 1:n
permutation = sample(indexRange, n)
startIndex = 1
for (i in 1:x) {
	stopIndex = startIndex + chunkSize - 1
	
	# Setting the indices for current fold
	test = permutation[startIndex:stopIndex]
	train = indexRange[-test]
	
	# Removing PRP from the training data
	computers_train = computers_df_norm[train, -7]
	computers_train_labels = computers_df_norm[train, 7]
	computers_test = computers_df_norm[test, -7]
	computers_test_labels = computers_df_norm[test, 7]
	
	best_k = best_k_for_knn_reg(
		computers_train,
		computers_train_labels,
		computers_test,
		computers_test_labels,
		kStart =  1,
		kEnd = 50
	)
	
	computers_knn =  knn.reg(
		train = computers_train,
		test = computers_test,
		y = computers_train_labels,
		k = best_k
	)
	mse = mean((computers_knn$pred - computers_test_labels) ^ 2)
	print(sprintf("Fold %d: Best k = %d with MSE = %f", i, best_k, mse))
	
	meanMSE = meanMSE + mse
	
	# Start index for next iteration
	startIndex = stopIndex + 1
}
mse_computers_knn = meanMSE / x
print(sprintf("k-NN regression with %d-fold CV: MSE = %f", x, mse_computers_knn))
```


## 1c. Compare the best model in (a) and the k-NN model you defined in (b). Which model do you prefer? Why? What is/are the advantage(s) of your choice? What about the drawbacks?

As can be seen, the k-NN model outperforms the linear regression model.
If we have enough memory, a k-NN model is (slightly) better here, but if we are limited (or have a huge amount of training data), the linear regression model would be more sensible. 

```{r}
print(sprintf("Single linear regression MSE = %f", mse_computers_single_linear))
print(sprintf("Multiple linear regression: MSE = %f", mse_computers_multi_linear))
print(sprintf("k-NN regression with 10-fold CV: MSE = %f", mse_computers_knn))
```


\newpage

## 2. Compare the two models (linear regression VS k-NN) for the Cars dataset (filename: Cars.txt) as it follows:

## 2d. Evaluate the quality of the fit (of the best model) between a single regression model of your choice and a multiple regression.

As we have already loaded the dataset (and removed the NAs), we will now exclude vendor, model, and ERP

```{r}
cars_df_cleaned_short = cars_df_cleaned[, 1:7]
```

Then normalize the dataframe:

```{r}
cars_df_norm = as.data.frame(lapply(cars_df_cleaned_short, normalize))
summary(cars_df_norm)
```

Creating the linear regressions:
```{r}
lm_cars_single = lm(mpg ~ weight, data = cars_df_norm)
summary(lm_cars_single)
lm_cars_multiple = lm(mpg ~ cylinders + displacement + horsepower + weight + acceleration + year,
											data = cars_df_norm)
summary(lm_cars_multiple)
```

Comparing them through the MSEs:
```{r}
cars_predict_single = cbind(cars_df_norm,
														predict(lm_cars_single, interval = 'confidence'))
cars_predict_multiple = cbind(cars_df_norm,
															predict(lm_cars_multiple, interval = 'confidence'))
mse_car_single_linear = mean((cars_predict_single$mpg - cars_predict_single$fit) ^
												 	2)
mse_car_multi_linear = mean((cars_predict_multiple$mpg - cars_predict_multiple$fit) ^ 2)
print(sprintf("Single linear regression MSE = %f", mse_car_single_linear))
print(sprintf("Multiple linear regression MSE = %f", mse_car_multi_linear))
```
Once again, multiple linear regression performs better than the single regression.

## 2e. Use the k-NN regression to build the second model, applying LOO or 10fold cross-validation.

We will be using 10-fold cross-validation:
```{r}
x = 10
n = nrow(computers_df_norm)
chunkSize = floor(n / x)
meanMSE = 0.0
indexRange = 1:n
permutation = sample(indexRange, n)
startIndex = 1
for (i in 1:x) {
	stopIndex = startIndex + chunkSize - 1
	
	# Setting the indices for current fold
	test = permutation[startIndex:stopIndex]
	train = indexRange[-test]
	
	# Removing mpg (first column) from the training data
	cars_train = cars_df_norm[train, -1]
	cars_train_labels = cars_df_norm[train, 1]
	cars_test = cars_df_norm[test, -1]
	cars_test_labels = cars_df_norm[test, 1]
	
	best_k = best_k_for_knn_reg(
		cars_train,
		cars_train_labels,
		cars_test,
		cars_test_labels,
		kStart =  1,
		kEnd = 50
	)
	
	cars_knn =  knn.reg(
		train = cars_train,
		test = cars_test,
		y = cars_train_labels,
		k = best_k
	)
	mse = mean((cars_knn$pred - cars_test_labels) ^ 2)
	print(sprintf("Fold %d: Best k = %d with MSE = %f", i, best_k, mse))
	
	meanMSE = meanMSE + mse
	
	# Start index for next iteration
	startIndex = stopIndex + 1
}
mse_cars_knn = meanMSE / x
print(sprintf("k-NN regression with %d-fold CV: MSE = %f", x, mse_cars_knn))
```

## 2f. Compare the best model in (d) and the k-nn model you defined in (e). Which model do you prefer? Why? What is/are the advantage(s) of your choice? What about the drawbacks?

Once again, k-NN regression has a lower MSE than both the single and multiple linear regressions.
If we have enough memory, a k-NN model is definitely better here (compared to the cars df), but if we are limited (or have a huge amount of training data), the linear regression model would be more sensible. 

```{r}
print(sprintf("Single linear regression MSE = %f", mse_car_single_linear))
print(sprintf("Multiple linear regression: MSE = %f", mse_car_multi_linear))
print(sprintf("k-NN regression with 10-fold CV: MSE = %f", mse_cars_knn))
```

