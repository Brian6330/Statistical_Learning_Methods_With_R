---
title: 'Exercise #9'
author: "Brian Schweigler; 16-102-071"
date: "19/05/2022"
output:
  pdf_document: default
  html_document: default
subtitle: Classification and Regression Trees
editor_options: 
  markdown: 
    wrap: 80
---
## Preliminaries
Load the required libraries

```{r}
library(tree)
```

Set a seed for later:
```{r}
set.seed(1786397)
```



Loading the low weight dataset, set Status as a factor and show an overview:

```{r}
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
weight_df = read.csv("LowWeight.txt", sep = "\t", header = TRUE)
summary(weight_df)
```

No NAs; the range of the values can't be gauged without further information.

Loading the heart dataset:
```{r}
heart_df = read.csv("Heart.txt",
										header = TRUE,
										sep = "\t",
										comment.char = "#")
summary(heart_df)
```

No NAs; the range of the values can't be gauged without further information.

## 1a. Create a regression tree using the variable birth_weight as a target. Plot the resulting model.

First we will remove the data that is not of interest in our predictions; being the ID and whether it is low birth weight (which is what we ant to predict)

```{r}
useful_weight <- weight_df[,-(1:2)]
summary(useful_weight)
```

Now we divide this into test (30%) and train (70%) data set:

```{r}
training = round(nrow(useful_weight) * 0.7)
training_index = sample(c(1:nrow(useful_weight)), training)
training_data = useful_weight[training_index,]
summary(training_data)
testing_data = useful_weight[-training_index,]
summary(testing_data)
```

Now we can perform the regression tree model:
```{r}
reg_tree_model <-
	tree(birth_weight ~ ., training_data, split = "deviance")
summary(reg_tree_model)
plot(reg_tree_model)
text(reg_tree_model, pretty = 0, cex = 1.1)
```

Here we have a tree, that will definitely need pruning.

\newpage

## 2. Calculate the train and test MSE. Describe the results you obtained.

This is quite straightforward for both sets:

```{r}
training_predictions <-
	predict(reg_tree_model, training_data, type = "vector")
training_MSE = mean ((training_predictions - training_data$birth_weight) ^
										 	2)
training_MSE

test_predictions <-
	predict(reg_tree_model, testing_data, type = "vector")
test_MSE = mean ((test_predictions - testing_data$birth_weight) ^ 2)
test_MSE
```

As the magnitude of the distance is data dependant, we can only say that the training MSE is of a factor 2.5 smaller than the MSE of the test predictions.

\newpage

## 3. Should your regression tree be pruned? If yes, which strategy would you use? Compare the previous test MSE with the one obtained with the pruned tree. Plot the new model.

Yes, definitely needs to be pruned (and will improve Test MSE as well):

```{r}
reg_tree_cv <- cv.tree(reg_tree_model, K = 10)
reg_tree_cv

dev_min = which(reg_tree_cv$dev == min(reg_tree_cv$dev))
dev_min_size = reg_tree_cv$size[dev_min]
plot(reg_tree_cv$size,
		 reg_tree_cv$dev,
		 main = "Deviance factor",
		 type = "b")
plot(reg_tree_cv)

reg_tree_pruned <-  prune.tree(reg_tree_model, best = 4)
summary(reg_tree_pruned)
plot(reg_tree_pruned)
text(reg_tree_pruned, pretty = 0)


test_predictions_2 <-
	predict(reg_tree_pruned, testing_data, type = "vector")
test_MSE_2 = mean ((test_predictions_2 - testing_data$birth_weight) ^ 2)
test_MSE_2

```

Comparing with the unpruned, we definitely have an improved MSE now:
```{r}
test_MSE
test_MSE_2
```


```{r}
plot(test_predictions_2, testing_data$birth_weight,
     main="Difference prediction and observed values",
     pch=20)
abline(0,1)
aMean <-  sqrt(mean((test_predictions_2 - testing_data$birth_weight)^2))
abline(h = test_MSE_2, lty="dotted", col="red")
```


\newpage

## 4. Create a classification tree using the variable disease as a target. Plot the resulting model.

First we will factorize disease (among others), and remove ID.

```{r}
heart_df$disease <-
	factor(
		heart_df$disease,
		levels = c(1, 2),
		labels = c("A", "P")
	)

heart_df$sex <-
	factor(
		heart_df$sex,
		levels = c(0, 1),
		labels = c("0", "1")
	)

heart_df$sugar <-
	factor(
		heart_df$sugar,
		levels = c(0, 1),
		labels = c("0", "1")
	)

useful_heart <- heart_df[, -1]
summary(useful_heart)
```

Now we divide this into test (30%) and train (70%) data set:

```{r}
training_h = round(nrow(useful_heart) * 0.7)
training_index_h = sample(c(1:nrow(useful_heart)), training_h)
training_data_h = useful_heart[training_index_h,]
summary(training_data_h)
testing_data_h = useful_heart[-training_index_h,]
summary(testing_data_h)
```


Now we can perform the regression tree model:
```{r}
reg_tree_model_h <-
	tree(as.factor(disease) ~ ., training_data_h, split = "deviance")
summary(reg_tree_model_h)
plot(reg_tree_model_h)
text(reg_tree_model_h, pretty = 0, cex = 1.1)
```

\newpage

## 5. Compute the confusion matrix for your model and calculate the accuracy, sensitivity and specificity. Describe the results you obtained.

First we'll need to make some predictions:

```{r}
Test_Output = predict(reg_tree_model_h, testing_data_h, type = "class")
Test_Error = mean(Test_Output != testing_data_h$disease)
Test_Error
```


```{r}
confusion_mat_h <-
	table(testing_data_h$disease, Test_Output)[2:1, 2:1]
confusion_mat_h
TP = confusion_mat_h[1]
TN = confusion_mat_h[4]
FP = confusion_mat_h[2]
FN = confusion_mat_h[3]

precision = TP / (TP + FP)
print(sprintf("Precision = %f", precision))

recall = TP / (TP + FN)
print(sprintf("Recall a.k.a. Sensitivity = %f", recall))

specificity = TN / (FP + TN)
print(sprintf("Specifcity = %f", specificity))


F1 = (2 * recall * precision) / (recall + precision)
print(sprintf("F1 measure = %f", F1))
```



## 6. Should your classification tree be pruned? If yes, which strategy would you use? Compare the previous results with the one obtained with the pruned tree. Plot the new model.

Yes, definitely should be pruned.

```{r}
reg_tree_cv_p <- cv.tree(reg_tree_model_h, K = 10)
reg_tree_cv_p

dev_min_p = which(reg_tree_cv_p$dev == min(reg_tree_cv_p$dev))
dev_min_size_p = reg_tree_cv_p$size[dev_min_p]
plot(reg_tree_cv_p$size,
		 reg_tree_cv_p$dev,
		 main = "Deviance factor",
		 type = "b")
plot(reg_tree_cv_p)

reg_tree_pruned_p <-  prune.tree(reg_tree_model_h, best = 4)
summary(reg_tree_pruned_p)
plot(reg_tree_pruned_p)
text(reg_tree_pruned_p, pretty = 0)
```

Now for some predictions first:

But first, some predictions:
```{r}
Test_Output_2 = predict(reg_tree_pruned_p, testing_data_h, type = "class")
Test_Error_2 = mean(Test_Output_2 != testing_data_h$disease)
Test_Error_2
```


```{r}
confusion_mat_p <-
	table(testing_data_h$disease, Test_Output_2)[2:1, 2:1]
confusion_mat_p
TP_p = confusion_mat_p[1]
TN_p = confusion_mat_p[4]
FP_p = confusion_mat_p[2]
FN_p = confusion_mat_p[3]

precision_p = TP_p / (TP_p + FP_p)
print(sprintf("Precision = %f", precision_p))

recall_p = TP_p / (TP_p + FN_p)
print(sprintf("Recall a.k.a. Sensitivity = %f", recall_p))

specificity_p = TN_p / (FP_p + TN_p)
print(sprintf("Specifcity = %f", specificity_p))


F1_p = (2 * recall_p * precision_p) / (recall_p + precision_p)
print(sprintf("F1 measure = %f", F1_p))
```

Everything except recall has improved after the pruning.
